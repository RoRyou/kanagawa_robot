# Package imports
import os
import pandas as pd
import re
import json
from time import sleep
from googleapiclient.discovery import build
import requests
from bs4 import BeautifulSoup
from markdownify import markdownify as md
import pdfplumber
from openai import AzureOpenAI
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import warnings
from typing import List, Tuple, Optional

warnings.simplefilter(action='ignore', category=FutureWarning)

# Config
os.environ["azure_api_key"] = "e87c4053cc814a338a2090130c2424c7"
os.environ["azure_api_version"] = "2023-06-01-preview"
os.environ["azure_azure_endpoint"] = "https://cog-openai-test-dev.openai.azure.com/"
os.environ["GOOGLE_API_KEY"] = "AIzaSyBkaSb5MwHK9gG-Q90xeRhuRAUq3p990Ts"
os.environ["CUSTOM_SEARCH_ENGINE_ID"] = "d057b4dee799c420d"

client = AzureOpenAI(
    api_version="2023-06-01-preview",
    api_key="e87c4053cc814a338a2090130c2424c7",
    azure_endpoint="https://cog-openai-test-dev.openai.azure.com/"
)

GOOGLE_API_KEY = "AIzaSyBkaSb5MwHK9gG-Q90xeRhuRAUq3p990Ts"
CUSTOM_SEARCH_ENGINE_ID = "d057b4dee799c420d"
DATA_DIR = 'data'

# =============================================================================
# åŸºæœ¬ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# =============================================================================

def clean_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    cleaned_text = re.sub(r'\n{2,}', '\n', text)
    cleaned_text = re.sub(r'\s{2,}', ' ', cleaned_text)
    cleaned_text = '\n'.join([line.strip() for line in cleaned_text.split('\n') if line.strip()])
    return cleaned_text

# =============================================================================
# æ¤œç´¢ãƒ»ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•°
# =============================================================================

def get_search_results(keyword, site_url=None, pages=5):
    """
    çµ±ä¸€çš„ãªGoogleæ¤œç´¢é–¢æ•°
    
    Args:
        keyword: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        site_url: é™å®šã‚µã‚¤ãƒˆURLï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        pages: è¿”ã™ãƒšãƒ¼ã‚¸æ•°ï¼ˆsite_urlãŒNoneã®å ´åˆã«ä½¿ç”¨ï¼‰
    
    Returns:
        list: URLãƒªã‚¹ãƒˆ
    """
    service = build("customsearch", "v1", developerKey=GOOGLE_API_KEY)
    urls = []
    
    if site_url:
        # siteä»˜ãæ¤œç´¢ï¼ˆå…ƒgetSearchResponseï¼‰
        page_limit = 10
        start_index = 1
        
        for n_page in range(0, page_limit):
            try:
                sleep(1)
                query = f'{keyword} site:{site_url}'
                res = service.cse().list(
                    q=query,
                    cx=CUSTOM_SEARCH_ENGINE_ID,
                    lr='lang_ja',
                    num=10,
                    start=start_index
                ).execute()
                
                urls.extend(item['link'] for item in res.get('items', []))
                
                # æ¬¡ã®ãƒšãƒ¼ã‚¸ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if "nextPage" in res.get("queries", {}):
                    start_index = res.get("queries").get("nextPage")[0].get("startIndex")
                else:
                    break
                    
            except Exception as e:
                print(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
                break
    else:
        # siteãªã—æ¤œç´¢ï¼ˆå…ƒgetSearchResponse_no_urlï¼‰
        try:
            sleep(1)
            res = service.cse().list(
                q=keyword,
                cx=CUSTOM_SEARCH_ENGINE_ID,
                lr='lang_ja',
                num=pages,
                start=1
            ).execute()
            
            urls.extend(item['link'] for item in res.get('items', []))
            
        except Exception as e:
            print(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
    
    return urls

def getTextFromUrl(url, timeout=10):
    try:
        response = requests.get(url, timeout=timeout)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        for element in soup(['header', 'footer', 'nav', 'aside', 'script', 'style']):
            element.decompose()
        
        text = soup.get_text(separator='\n')
        return clean_text(text.strip())
    except requests.exceptions.Timeout:
        print(f'Timeout occurred while fetching {url}')
        return None
    except Exception as e:
        print(f'Error fetching {url}: {e}')
        return None

def get_pdf_text(url, pagenum=5, timeout=10):
    try:
        response = requests.get(url, timeout=timeout)
        response.raise_for_status()
        with open('temp.pdf', 'wb') as temp_pdf:
            temp_pdf.write(response.content)
        with pdfplumber.open('temp.pdf') as pdf:
            text = ''
            for num, page in enumerate(pdf.pages):
                if num >= pagenum:
                    break
                text += page.extract_text() + '\n'
        os.remove('temp.pdf')
        return text
    except requests.exceptions.Timeout:
        print(f'Timeout occurred while fetching PDF {url}')
        return None
    except Exception as e:
        print(f'Error extracting text from PDF {url}: {e}')
        return None

def getAllTextFromUrls(urls):
    all_text = {}
    for url in urls:
        print("ãƒ‡ãƒ¼ã‚¿å–å¾—:", url)
        if url.endswith('.pdf'):
            text = get_pdf_text(url, 5)
        else:
            text = getTextFromUrl(url)
        if text:
            all_text[url] = clean_text(text)
    return all_text

# =============================================================================
# GPTå‡¦ç†é–¢æ•°
# =============================================================================

def make_gpt_prompt(input_text, company, product=None):
    """
    GPTãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹çµ±ä¸€é–¢æ•°
    
    Args:
        input_text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
        company: ä¼šç¤¾å
        product: è£½å“åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€Phase 3ã§ä½¿ç”¨ï¼‰
    
    Returns:
        str: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    """
    base_prompt = f"""/system

ã‚ãªãŸã¯ãƒ—ãƒ­ã®ãƒªã‚µãƒ¼ãƒãƒ»ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã€ç§ã®ä»•äº‹ã‚’æ‰‹ä¼ã†ã“ã¨ã§ã™ã€‚
ç§ã®ä»•äº‹ã¯ã€è«–ç‚¹ã‚’æ•´ç†ã—ãŸãã‚Œã„ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹ã“ã¨ã§ã™ã€‚
ã“ã‚Œã‹ã‚‰ã€ãƒ­ãƒœãƒƒãƒˆã®è£½å“ã«é–¢ã™ã‚‹è¨˜äº‹ã‚’æŒ™ã’ã¾ã™ã€‚"""

    if product is None:
        # Phase 1: è£½å“è­˜åˆ¥
        prompt = base_prompt + f"""
è¨˜äº‹ã®ä¸­ã‹ã‚‰ã€{company}ã®ãƒ­ãƒœãƒƒãƒˆè£½å“ã«ã¤ã„ã¦ã€ãƒ­ãƒœãƒƒãƒˆãŒä»¥ä¸‹ã®#åˆ†é¡é …ç›®ã«å½“ã¦ã¯ã¾ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚
#åˆ†é¡é …ç›®
[ç§»å‹•ä½œæ¥­å‹ãƒ­ãƒœãƒƒãƒˆ,äººé–“è£…ç€å‹ãƒ­ãƒœãƒƒãƒˆ,æ­ä¹—å‹ãƒ­ãƒœãƒƒãƒˆ,ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³å‹ãƒ­ãƒœãƒƒãƒˆ,æ±ç”¨å‹ãƒ­ãƒœãƒƒãƒˆ,ç”£æ¥­ç”¨ãƒ­ãƒœãƒƒãƒˆ]
ã‚ã¦ã¯ã¾ã‚‹å ´åˆã€ãƒ­ãƒœãƒƒãƒˆã®åˆ†é¡ã€ä½¿ç”¨ç”¨é€”ã¨æŠ€è¡“é ˜åŸŸã€å”æ¥­å®Ÿç¸¾ã€è£½å“æƒ…å ±ã‚’ãã¡ã‚“ã¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸæ–‡å­—åˆ—å½¢å¼ï¼ˆstringsï¼‰ã®JSONã§ã€è¿”ã—ã¦ãã ã•ã„ã€‚
ä½¿ç”¨ç”¨é€”ã®ä¾‹ã¯ä»¥ä¸‹ã«ãªã‚Šã¾ã™ã€‚
[æ¸…æƒã€è­¦å‚™ã€æ¡ˆå†…ã€é…è†³ã€æ¬é€ã€è‡ªå‹•é‹è»¢è»Šã€ãƒ‰ãƒ­ãƒ¼ãƒ³ã€ãƒãƒƒã‚¹ãƒ«ã‚¹ãƒ¼ãƒ„ã€ã‚¢ã‚·ã‚¹ãƒˆã‚¹ãƒ¼ãƒ„ã€è»Šã„ã™ã€ãƒ¢ãƒ“ãƒªãƒ†ã‚£ã€ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒšãƒƒãƒˆã€äººå‹ãƒ­ãƒœãƒƒãƒˆã€ãã®ä»–]
è¨˜äº‹ã«è©²å½“ã™ã‚‹ã‚‚ã®ãŒãªã„å ´åˆã¯ç©ºç™½ã§è¿”ã—ã¦ãã ã•ã„
å‡ºåŠ›ã¯JSONã®ã¿ã«ã—ã¦ãã ã•ã„

å‡ºåŠ›ã®ä¾‹ã§ã™ã€‚
{{
    "åˆ†é¡": "ç§»å‹•ä½œæ¥­å‹ãƒ­ãƒœãƒƒãƒˆ",
    "ä½¿ç”¨ç”¨é€”": "é…è†³",
    "æŠ€è¡“é ˜åŸŸ": "åŒ»ç™‚",
    "å”æ¥­å®Ÿç¸¾": "æ ªå¼ä¼šç¤¾ãƒˆãƒ¨ã‚¿",
    "è£½å“æƒ…å ±": "å°å‹ä¸æ•´åœ°ç§»å‹•ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¦ãƒ‹ãƒƒãƒˆ"
}}"""
    else:
        # Phase 3: è£½å“è©³ç´°
        prompt = base_prompt + f"""
è¨˜äº‹ã®ä¸­ã«ã€{company}ã®{product}ã®ãƒ­ãƒœãƒƒãƒˆè£½å“ã«é–¢ã™ã‚‹æƒ…å ±ãŒå­˜åœ¨ã™ã‚‹ã‹ã©ã†ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚
å­˜åœ¨ã™ã‚‹å ´åˆã€ä½¿ç”¨ç”¨é€”ã¨æŠ€è¡“é ˜åŸŸã€å”æ¥­å®Ÿç¸¾ã€è£½å“æƒ…å ±ã€å®Ÿè¨¼å®Ÿé¨“ã€è£½å“ã®èª¬æ˜ã‚’ãã¡ã‚“ã¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸæ–‡å­—åˆ—å½¢å¼ï¼ˆstringsï¼‰ã®JSONã§ã€è¿”ã—ã¦ãã ã•ã„ã€‚
ä½¿ç”¨ç”¨é€”ã®ä¾‹ã¯ä»¥ä¸‹ã«ãªã‚Šã¾ã™ã€‚
[æ¸…æƒã€è­¦å‚™ã€æ¡ˆå†…ã€é…è†³ã€æ¬é€ã€è‡ªå‹•é‹è»¢è»Šã€ãƒ‰ãƒ­ãƒ¼ãƒ³ã€ãƒãƒƒã‚¹ãƒ«ã‚¹ãƒ¼ãƒ„ã€ã‚¢ã‚·ã‚¹ãƒˆã‚¹ãƒ¼ãƒ„ã€è»Šã„ã™ã€ãƒ¢ãƒ“ãƒªãƒ†ã‚£ã€ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒšãƒƒãƒˆã€äººå‹ãƒ­ãƒœãƒƒãƒˆã€ãã®ä»–]
æŠ€è¡“é ˜åŸŸã®ä¾‹ã¯ä»¥ä¸‹ã«ãªã‚Šã¾ã™ã€‚
[åŒ»ç™‚ã€ã‚¤ãƒ³ãƒ•ãƒ©ã€èˆªç©ºãƒ»å®‡å®™ã€ä»‹è­·ãƒ»ç¦ç¥‰ã€ç‰©æµãƒ»é‹é€ã€è¾²æ—æ°´ç”£æ¥­ã€å•†æ¥­æ–½è¨­ãƒ»å®¿æ³Šæ–½è¨­ã€ãã®ä»–]
è¨˜äº‹ã«è©²å½“ã™ã‚‹ã‚‚ã®ãŒãªã„å ´åˆã¯ç©ºç™½ã§è¿”ã—ã¦ãã ã•ã„
å‡ºåŠ›ã¯JSONã®ã¿ã«ã—ã¦ãã ã•ã„

å‡ºåŠ›ã®ä¾‹ã§ã™ã€‚
{{
    "ä¼æ¥­å": "{company}",
    "è£½å“å": "{product}",
    "åˆ†é¡": "ç§»å‹•ä½œæ¥­å‹ãƒ­ãƒœãƒƒãƒˆ",
    "ä½¿ç”¨ç”¨é€”": "é…è†³",
    "æŠ€è¡“é ˜åŸŸ": "åŒ»ç™‚",
    "å”æ¥­å®Ÿç¸¾": "æ ªå¼ä¼šç¤¾ãƒˆãƒ¨ã‚¿",
    "è£½å“æƒ…å ±": "å°å‹ä¸æ•´åœ°ç§»å‹•ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¦ãƒ‹ãƒƒãƒˆ",
    "å®Ÿè¨¼å®Ÿé¨“": "åŒ»ç™‚æ³•äººXYZç—…é™¢ã«ã¦ã€2024å¹´4æœˆã‹ã‚‰6æœˆã¾ã§ã®3ãƒ¶æœˆé–“ã€ç—…é™¢å†…ã§ã®é£Ÿäº‹ã‚„åŒ»è–¬å“ã®é…é”ã‚’è¡Œã†å®Ÿè¨¼å®Ÿé¨“ã‚’å®Ÿæ–½",
    "è£½å“ã®èª¬æ˜": "ã“ã‚Œã¯æœ€æ–°ã®åŒ»ç™‚ç”¨é…è†³ãƒ­ãƒœãƒƒãƒˆã§ã€ä¸æ•´åœ°ã§ã‚‚å®‰å®šã—ãŸç§»å‹•ãŒå¯èƒ½ã§ã™ã€‚"
}}"""
    
    return prompt + f"\n\n/è¨˜äº‹\n\n{input_text}"

def get_json_from_response_gpt4o(response):
    try:
        obj = json.loads(response)
        return obj, None
    except:
        try:
            out = json.loads(response.strip('```json\n').strip('\n```'))
            return out, None
        except:
            try:
                text = response.replace('json', '').replace('`', '')
                json_array_str = f'[{text}]'
                json_array_str = re.sub(r'\n', '', json_array_str)
                out = json.loads(json_array_str)
                return out, None
            except:
                print("Response was:", response)
                return [], response

def extract_arguments_gpt(input_text, company, product=None, return_raw=False):
    """
    çµ±ä¸€çš„ãªGPTå¼•æ•°æŠ½å‡ºé–¢æ•°
    
    Args:
        input_text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
        company: ä¼šç¤¾å
        product: è£½å“åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        return_raw: ç”Ÿã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ–‡å­—åˆ—ã‚’è¿”ã™ã‹ã©ã†ã‹
    
    Returns:
        return_raw=Trueã®å ´åˆ: ç”Ÿã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ–‡å­—åˆ—
        ãã‚Œä»¥å¤–: (è§£ææ¸ˆã¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸)ã®ã‚¿ãƒ—ãƒ«
    """
    prompt = make_gpt_prompt(input_text, company, product)
    
    result_llm = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "system", "content": prompt}]
    )
    
    response = result_llm.choices[0].message.content
    
    if return_raw:
        return response
    else:
        return get_json_from_response_gpt4o(response)

# =============================================================================
# Phase 1-4 ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–¢æ•°
# =============================================================================

def phase1_collect_urls(company_list, max_workers=3):
    """æ®µéš1: å…¨ä¼æ¥­ã®URLåé›†"""
    print("ğŸ“¡ æ®µéš1: å…¨ä¼æ¥­ã®URLåé›†ä¸­...")
    stage1_start = time.time()
    
    robo_category_list = ["ç§»å‹•ä½œæ¥­å‹ãƒ­ãƒœãƒƒãƒˆ", "äººé–“è£…ç€å‹ãƒ­ãƒœãƒƒãƒˆ", "æ­ä¹—å‹ãƒ­ãƒœãƒƒãƒˆ",
                         "ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³å‹ãƒ­ãƒœãƒƒãƒˆ", "æ±ç”¨å‹ãƒ­ãƒœãƒƒãƒˆ", "ç”£æ¥­ç”¨ãƒ­ãƒœãƒƒãƒˆ"]
    
    all_company_urls = {}
    
    def collect_urls_for_company(company):
        try:
            print(f"URLåé›†ä¸­: {company}")
            urls = set()
            for robo in robo_category_list:
                try:
                    keyword = robo + 'ã€€' + company
                    urls_1 = get_search_results(keyword)  # çµ±ä¸€é–¢æ•°ã‚’ä½¿ç”¨
                    urls.update(urls_1)
                except Exception as e:
                    print(f"  âœ— {company} - {robo}: {e}")
                    continue
            return company, list(urls)
        except Exception as e:
            print(f"âœ— {company} URLåé›†ã‚¨ãƒ©ãƒ¼: {e}")
            return company, []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(collect_urls_for_company, company) for company in company_list]
        for future in tqdm(as_completed(futures), total=len(futures), desc="ä¼æ¥­URLåé›†"):
            company, urls = future.result()
            all_company_urls[company] = urls
    
    # çµ±è¨ˆå‡¦ç†
    total_urls = sum(len(urls) for urls in all_company_urls.values())
    stage1_time = time.time() - stage1_start
    
    print(f"âœ… æ®µéš1å®Œäº†: {stage1_time:.1f}ç§’")
    print(f"åˆè¨ˆ: {total_urls} URLs")
    
    # URL DataFrameä½œæˆ
    url_list_data = []
    for company, urls in all_company_urls.items():
        for url in urls:
            url_list_data.append({'ä¼æ¥­å': company, 'URL': url})
    
    url_df = pd.DataFrame(url_list_data) if url_list_data else pd.DataFrame(columns=['ä¼æ¥­å', 'URL'])
    
    stats = {
        'total_companies': len(company_list),
        'total_urls': total_urls,
        'processing_time': stage1_time,
        'urls_per_company': {company: len(urls) for company, urls in all_company_urls.items()}
    }
    
    return all_company_urls, url_df, stats

def phase2_get_texts(all_company_urls, max_workers=3):
    """æ®µéš2: å…¨URLã®ãƒ†ã‚­ã‚¹ãƒˆå–å¾—"""
    print("ğŸ“„ æ®µéš2: å…¨URLã®ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ä¸­...")
    stage2_start = time.time()
    
    total_urls = sum(len(urls) for urls in all_company_urls.values())
    if total_urls == 0:
        print("âŒ URLãŒå­˜åœ¨ã—ã¾ã›ã‚“")
        return {}, {'total_urls': 0, 'total_texts': 0, 'processing_time': 0}
    
    all_company_texts = {}
    
    def get_texts_for_company(company_data):
        company, urls = company_data
        try:
            print(f"ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ä¸­: {company} ({len(urls)} URLs)")
            all_text = getAllTextFromUrls(urls)
            print(f"  âœ“ {company}: {len(all_text)}/{len(urls)} æˆåŠŸå–å¾—")
            return company, all_text
        except Exception as e:
            print(f"âœ— {company} ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return company, {}
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [
            executor.submit(get_texts_for_company, (company, urls))
            for company, urls in all_company_urls.items()
            if len(urls) > 0
        ]
        
        for future in tqdm(as_completed(futures), total=len(futures), desc="ãƒ†ã‚­ã‚¹ãƒˆå–å¾—"):
            company, texts = future.result()
            all_company_texts[company] = texts
    
    # çµ±è¨ˆå‡¦ç†
    total_texts = sum(len(texts) for texts in all_company_texts.values())
    stage2_time = time.time() - stage2_start
    
    print(f"âœ… æ®µéš2å®Œäº†: {stage2_time:.1f}ç§’")
    print(f"åˆè¨ˆ: {total_texts} ãƒ†ã‚­ã‚¹ãƒˆ")
    
    stats = {
        'total_urls': total_urls,
        'total_texts': total_texts,
        'processing_time': stage2_time,
        'texts_per_company': {company: len(texts) for company, texts in all_company_texts.items()}
    }
    
    return all_company_texts, stats

def phase3_gpt_processing(all_company_texts, max_workers=3):
    """æ®µéš3: å…¨ãƒ†ã‚­ã‚¹ãƒˆã®GPTå‡¦ç†"""
    print("ğŸ¤– æ®µéš3: å…¨ãƒ†ã‚­ã‚¹ãƒˆã®GPTå‡¦ç†ä¸­...")
    stage3_start = time.time()
    
    total_texts = sum(len(texts) for texts in all_company_texts.values())
    if total_texts == 0:
        print("âŒ ãƒ†ã‚­ã‚¹ãƒˆãŒå­˜åœ¨ã—ã¾ã›ã‚“")
        return [], {'total_texts': 0, 'successful_gpt': 0, 'processing_time': 0}
    
    def process_single_url_gpt(url_data):
        company, url, text = url_data
        try:
            if len(text) > 12000:
                text = text[:12000]
            
            # çµ±ä¸€é–¢æ•°ã‚’ä½¿ç”¨
            result, error = extract_arguments_gpt(text, company)
            
            return {
                'company': company,
                'url': url,
                'result': result,
                'error': error,
                'status': 'success' if result else 'no_result'
            }
        except Exception as e:
            print(f"GPTå‡¦ç†ä¾‹å¤–ã‚¨ãƒ©ãƒ¼: {url} - {e}")
            return {
                'company': company,
                'url': url,
                'result': None,
                'error': str(e),
                'status': 'failed'
            }
    
    # ã‚¿ã‚¹ã‚¯ä½œæˆ
    all_gpt_tasks = []
    for company, texts in all_company_texts.items():
        for url, text in texts.items():
            if text:
                all_gpt_tasks.append((company, url, text))
    
    print(f"GPTå‡¦ç†ã‚¿ã‚¹ã‚¯æ•°: {len(all_gpt_tasks)}")
    
    # ä¸¦åˆ—GPTå‡¦ç†
    gpt_results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_single_url_gpt, task) for task in all_gpt_tasks]
        
        for future in tqdm(as_completed(futures), total=len(futures), desc="GPTå‡¦ç†"):
            result = future.result()
            gpt_results.append(result)
    
    # çµ±è¨ˆå‡¦ç†
    successful_gpt = [r for r in gpt_results if r['status'] == 'success' and r['result']]
    stage3_time = time.time() - stage3_start
    
    print(f"âœ… æ®µéš3å®Œäº†: {stage3_time:.1f}ç§’")
    print(f"  æˆåŠŸ: {len(successful_gpt)}")
    
    stats = {
        'total_texts': total_texts,
        'total_tasks': len(all_gpt_tasks),
        'successful_gpt': len(successful_gpt),
        'processing_time': stage3_time,
        'success_rate': len(successful_gpt) / len(all_gpt_tasks) * 100 if all_gpt_tasks else 0
    }
    
    return gpt_results, stats

def phase4_organize_data(gpt_results):
    """æ®µéš4: ãƒ‡ãƒ¼ã‚¿æ•´ç†"""
    print("ğŸ“‹ æ®µéš4: ãƒ‡ãƒ¼ã‚¿æ•´ç†ä¸­...")
    stage4_start = time.time()
    
    successful_gpt = [r for r in gpt_results if r['status'] == 'success' and r['result']]
    
    if not successful_gpt:
        print("âŒ æ•´ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
        return pd.DataFrame(), {'total_records': 0, 'processing_time': 0}
    
    # ãƒ‡ãƒ¼ã‚¿å¹³å¦åŒ–
    flattened_data = []
    for gpt_result in successful_gpt:
        url = gpt_result['url']
        company = gpt_result['company']
        result = gpt_result['result']
        
        try:
            if isinstance(result, list):
                for entry in result:
                    if isinstance(entry, dict):
                        entry['URL'] = url
                        entry['ä¼æ¥­å'] = company
                        flattened_data.append(entry)
            elif isinstance(result, dict):
                result['URL'] = url
                result['ä¼æ¥­å'] = company
                flattened_data.append(result)
        except Exception as e:
            print(f"ãƒ‡ãƒ¼ã‚¿å¹³å¦åŒ–ã‚¨ãƒ©ãƒ¼: {url} - {e}")
    
    df_result = pd.DataFrame(flattened_data) if flattened_data else pd.DataFrame()
    
    stage4_time = time.time() - stage4_start
    
    print(f"âœ… æ®µéš4å®Œäº†: {stage4_time:.1f}ç§’")
    print(f"ğŸ“Š æœ€çµ‚ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df_result)}")
    
    stats = {
        'total_records': len(df_result),
        'companies_with_data': df_result['ä¼æ¥­å'].nunique() if not df_result.empty else 0,
        'processing_time': stage4_time,
        'records_per_company': df_result['ä¼æ¥­å'].value_counts().to_dict() if not df_result.empty else {}
    }
    
    return df_result, stats

# =============================================================================
# Phase 3 ç‰¹åˆ¥å‡¦ç†é–¢æ•°
# =============================================================================

def process_single_row_final_df3(row_data):
    """å˜ä¸€è¡Œãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†é–¢æ•°"""
    index = row_data['index']
    product = row_data['è£½å“æƒ…å ±']
    company = row_data['ä¼æ¥­å']
    
    try:
        search_query = f"{product} {company}"
        urls = get_search_results(search_query, pages=1)  # çµ±ä¸€é–¢æ•°ã‚’ä½¿ç”¨
        
        if len(urls) == 0:
            return {
                'index': index,
                'url': "",
                'text': "",
                'status': 'no_urls',
                'error': "No URLs found"
            }
        
        all_text = getAllTextFromUrls(urls)
        url = urls[0]
        
        if url not in all_text or not all_text[url]:
            return {
                'index': index,
                'url': url,
                'text': "",
                'status': 'no_text',
                'error': "No text content found"
            }
        
        text_content = all_text[url]
        
        # GPTå‡¦ç†
        max_attempts = 2
        for attempt in range(max_attempts):
            try:
                df_text = extract_arguments_gpt(text_content, company, product, return_raw=True)  # çµ±ä¸€é–¢æ•°ã‚’ä½¿ç”¨
                df_text_1 = json.loads(df_text.replace("```json", "").replace("```", "").strip())
                
                return {
                    'index': index,
                    'url': url,
                    'text': df_text_1,
                    'status': 'success',
                    'attempt': attempt + 1
                }
            except Exception as gpt_error:
                if attempt < max_attempts - 1:
                    print(f"  Row {index}: {attempt+1}å›ç›®ã®è©¦è¡Œã«å¤±æ•—ã€‚å†è©¦è¡Œã—ã¾ã™...")
                    continue
                else:
                    return {
                        'index': index,
                        'url': url,
                        'text': "",
                        'status': 'gpt_failed',
                        'error': str(gpt_error),
                        'attempt': attempt + 1
                    }
    except Exception as e:
        return {
            'index': index,
            'url': "",
            'text': "",
            'status': 'failed',
            'error': str(e)
        }

def process_final_df3_parallel(final_df3, max_workers=3):
    """final_df3ã®ä¸¦åˆ—å‡¦ç†"""
    print(f"ğŸš€ final_df3 ä¸¦åˆ—å‡¦ç†é–‹å§‹ (workers: {max_workers})")
    start_time = time.time()
    
    final_df3_copy = final_df3.copy()
    final_df3_copy["url"] = ""
    final_df3_copy["text"] = ""
    
    # æœ‰åŠ¹è¡Œãƒ‡ãƒ¼ã‚¿ã®åé›†
    valid_rows = []
    for n in range(len(final_df3_copy)):
        product = final_df3_copy["è£½å“æƒ…å ±"].iloc[n]
        company = final_df3_copy["ä¼æ¥­å"].iloc[n]
        
        if not (pd.isna(product) or pd.isna(company)):
            valid_rows.append({
                'index': n,
                'è£½å“æƒ…å ±': product,
                'ä¼æ¥­å': company
            })
    
    print(f"ğŸ“Š å‡¦ç†å¯¾è±¡: {len(valid_rows)}/{len(final_df3_copy)} è¡Œ")
    
    if len(valid_rows) == 0:
        print("âŒ å‡¦ç†å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return final_df3_copy, [], {'total_rows': len(final_df3_copy), 'processed': 0}
    
    # ä¸¦åˆ—å‡¦ç†
    results = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_single_row_final_df3, row_data) for row_data in valid_rows]
        
        for future in tqdm(as_completed(futures), total=len(futures), desc="Processing rows"):
            result = future.result()
            results.append(result)
    
    # çµæœã®çµ±è¨ˆ
    successful_results = [r for r in results if r['status'] == 'success']
    failed_results = [r for r in results if r['status'] != 'success']
    
    # DataFrameã®æ›´æ–°
    for result in successful_results:
        index = result['index']
        final_df3_copy.at[index, "url"] = result['url']
        final_df3_copy.at[index, "text"] = result['text']
    
    # å¤±æ•—ã‚±ãƒ¼ã‚¹ã®åé›†
    failed_cases = []
    for result in failed_results:
        failed_cases.append((result['index'], result.get('error', 'Unknown error')))
    
    processing_time = time.time() - start_time
    
    # çµ±è¨ˆæƒ…å ±
    statistics = {
        'total_rows': len(final_df3_copy),
        'valid_rows': len(valid_rows),
        'successful': len(successful_results),
        'failed': len(failed_results),
        'success_rate': len(successful_results) / len(valid_rows) * 100 if valid_rows else 0,
        'processing_time': processing_time,
        'rows_per_second': len(valid_rows) / processing_time if processing_time > 0 else 0
    }
    
    print(f"âœ… å‡¦ç†å®Œäº†: {processing_time:.1f}ç§’")
    print(f"ğŸ“Š å‡¦ç†çµ±è¨ˆ: æˆåŠŸ: {statistics['successful']}, å¤±æ•—: {statistics['failed']}")
    
    return final_df3_copy, failed_cases, statistics

# =============================================================================
# JSONå±•é–‹ãƒ»ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–¢æ•°
# =============================================================================

def expand_json_simple(df, text_column='text', prefix='json_'):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªJSONå±•é–‹"""
    column_list = ['ä¼æ¥­å', 'è£½å“å', 'åˆ†é¡', 'ä½¿ç”¨ç”¨é€”', 'æŠ€è¡“é ˜åŸŸ',
                   'å”æ¥­å®Ÿç¸¾', 'è£½å“æƒ…å ±', 'å®Ÿè¨¼å®Ÿé¨“', 'è£½å“ã®èª¬æ˜']
    
    df_result = df.copy()
    
    # æ–°ã—ã„åˆ—ã‚’åˆæœŸåŒ–
    for col in column_list:
        col_name = f"{prefix}{col}" if prefix else col
        df_result[col_name] = ''
    
    # å„è¡Œã‚’å‡¦ç†
    for idx, row in tqdm(df_result.iterrows(), total=len(df_result), desc="JSONå±•é–‹ä¸­"):
        data = row[text_column]
        
        if pd.isna(data) or data == '':
            continue
            
        # è¾æ›¸ã¾ãŸã¯æ–‡å­—åˆ—ã‚’å‡¦ç†
        if isinstance(data, dict):
            parsed_data = data
        elif isinstance(data, str):
            try:
                parsed_data = json.loads(data.replace("'", '"').strip())
            except:
                continue
        else:
            continue
        
        # å„åˆ—ã®å€¤ã‚’è¨­å®š
        for col in column_list:
            col_name = f"{prefix}{col}" if prefix else col
            if col in parsed_data:
                df_result.at[idx, col_name] = parsed_data[col]
    
    return df_result

def clean_product_batch_simple(product_list):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªè£½å“åãƒãƒƒãƒã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    if not product_list:
        return []
    
    indexed_items = {str(i): item for i, item in enumerate(product_list)}
    
    prompt = f"""ä»¥ä¸‹ã®è£½å“åã®è¡¨è¨˜æºã‚Œã‚’çµ±ä¸€ã—ã¦ãã ã•ã„ã€‚
å…¥åŠ›: {indexed_items}
å‡ºåŠ›: JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚
ä¾‹: {{"0": "çµ±ä¸€ã•ã‚ŒãŸè£½å“å1", "1": "çµ±ä¸€ã•ã‚ŒãŸè£½å“å2"}}
æ³¨æ„: å¿…ãšåŒã˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·ã§è¿”ã—ã¦ãã ã•ã„ã€‚"""
    
    try:
        result_llm = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0
        )
        
        response = result_llm.choices[0].message.content.strip()
        if response.startswith('```json'):
            response = response.replace('```json', '').replace('```', '').strip()
        
        result_dict = json.loads(response)
        
        cleaned_list = []
        for i in range(len(product_list)):
            cleaned_list.append(result_dict.get(str(i), product_list[i]))
        
        return cleaned_list
        
    except:
        return product_list

def clean_dataframe_products(df, column_name='json_è£½å“å', batch_size=10):
    """DataFrameã®è£½å“ååˆ—ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
    print(f"ğŸ§¹ {column_name} ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ä¸­...")
    
    non_empty_mask = df[column_name].notna() & (df[column_name] != '') & (df[column_name] != '{}')
    non_empty_products = df.loc[non_empty_mask, column_name].tolist()
    
    if len(non_empty_products) == 0:
        print("âŒ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡ãªã—")
        df[f'{column_name}_cleaned'] = df[column_name]
        return df
    
    unique_products = list(set(non_empty_products))
    print(f"ğŸ“Š {len(unique_products)} ãƒ¦ãƒ‹ãƒ¼ã‚¯è£½å“åã‚’å‡¦ç†")
    
    cleaned_mapping = {}
    for i in tqdm(range(0, len(unique_products), batch_size), desc="ãƒãƒƒãƒå‡¦ç†"):
        batch = unique_products[i:i + batch_size]
        cleaned_batch = clean_product_batch_simple(batch)
        
        for original, cleaned in zip(batch, cleaned_batch):
            cleaned_mapping[original] = cleaned
    
    df[f'{column_name}_cleaned'] = df[column_name].map(cleaned_mapping).fillna(df[column_name])
    
    changed_count = sum(1 for orig, clean in cleaned_mapping.items() if orig != clean)
    print(f"âœ… å®Œäº†: {changed_count}/{len(unique_products)} ä»¶å¤‰æ›´")
    
    return df

def extract_datazora_categories(input_list):
    """æŠ€è¡“é ˜åŸŸã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡"""
    if not input_list:
        return []
        
    valid_categories = {
        "åŒ»ç™‚", "ã‚¤ãƒ³ãƒ•ãƒ©", "èˆªç©ºãƒ»å®‡å®™", "ä»‹è­·ãƒ»ç¦ç¥‰", 
        "ç‰©æµãƒ»æ¬é€", "è¾²æ—æ°´ç”£æ¥­", "å•†æ¥­æ–½è¨­ãƒ»å®¿æ³Šæ–½è¨­"
    }
    
    categories = [
        "åŒ»ç™‚", "ã‚¤ãƒ³ãƒ•ãƒ©", "èˆªç©ºãƒ»å®‡å®™", "ä»‹è­·ãƒ»ç¦ç¥‰",
        "ç‰©æµãƒ»æ¬é€", "è¾²æ—æ°´ç”£æ¥­", "å•†æ¥­æ–½è¨­ãƒ»å®¿æ³Šæ–½è¨­", "ãã®ä»–"
    ]
    
    prompt = f"""ä»¥ä¸‹ã®ãƒªã‚¹ãƒˆã®å„é …ç›®ã‚’ã€æŒ‡å®šã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªãƒ¼ã«åˆ†é¡ã—ã¦ãã ã•ã„ã€‚

å…¥åŠ›: {input_list}

ã‚«ãƒ†ã‚´ãƒªãƒ¼:
{chr(10).join(f'{i+1}. {cat}' for i, cat in enumerate(categories))}

åˆ†é¡ãƒ«ãƒ¼ãƒ«:
1. å„é …ç›®ã¯è¤‡æ•°ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«åˆ†é¡å¯èƒ½ã§ã™
2. è¤‡æ•°ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«å½“ã¦ã¯ã¾ã‚‹å ´åˆã¯ã€"/"ã§åŒºåˆ‡ã£ã¦åˆ—æŒ™ã—ã¦ãã ã•ã„
3. ã©ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«ã‚‚å½“ã¦ã¯ã¾ã‚‰ãªã„å ´åˆã¯ã€Œãã®ä»–ã€ã«åˆ†é¡ã—ã¦ãã ã•ã„

å¿…ãšå…¥åŠ›ã¨åŒã˜é•·ã•ã®Pythonãƒªã‚¹ãƒˆå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚
ä¾‹: ['åŒ»ç™‚/ä»‹è­·ãƒ»ç¦ç¥‰', 'ã‚¤ãƒ³ãƒ•ãƒ©', 'ãã®ä»–']"""
    
    try:
        result_llm = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system", 
                    "content": "Pythonã®ãƒªã‚¹ãƒˆå½¢å¼ã§ã®ã¿è¿”ç­”ã—ã¦ãã ã•ã„ã€‚èª¬æ˜æ–‡ã¯ä¸è¦ã§ã™ã€‚"
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ]
        )
        
        response = result_llm.choices[0].message.content.strip()
        categorized_list = eval(response)
        
        cleaned_list = []
        for item in categorized_list:
            if pd.isna(item) or not item:
                cleaned_list.append('ãã®ä»–')
                continue
                
            categories = item.split('/')
            valid_cats = [cat for cat in categories if cat in valid_categories]
            
            if valid_cats:
                cleaned_list.append('/'.join(valid_cats))
            else:
                cleaned_list.append('ãã®ä»–')
        
        return cleaned_list
        
    except Exception as e:
        print(f"Error parsing response: {e}")
        return ['ãã®ä»–'] * len(input_list)

def add_skill_categories_to_dataframe(df, target_column='æŠ€è¡“é ˜åŸŸ', batch_size=10):
    """DataFrameã«æŠ€è¡“é ˜åŸŸã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’è¿½åŠ """
    print(f"ğŸ”– æŠ€è¡“é ˜åŸŸã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡é–‹å§‹...")
    
    df_result = df.copy()
    
    if target_column not in df_result.columns:
        print(f"âŒ åˆ— '{target_column}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
        return df_result
    
    valid_data = df_result[target_column].dropna().tolist()
    unique_data = list(set(valid_data))
    
    print(f"ğŸ“Š å‡¦ç†å¯¾è±¡: {len(unique_data)} ãƒ¦ãƒ‹ãƒ¼ã‚¯æŠ€è¡“é ˜åŸŸ")
    
    if len(unique_data) == 0:
        df_result[f'{target_column}_category'] = 'ãã®ä»–'
        return df_result
    
    category_mapping = {}
    
    for i in tqdm(range(0, len(unique_data), batch_size), desc="ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡"):
        batch = unique_data[i:i + batch_size]
        categorized_batch = extract_datazora_categories(batch)
        
        for original, category in zip(batch, categorized_batch):
            category_mapping[original] = category
    
    df_result[f'{target_column}_category'] = df_result[target_column].map(category_mapping).fillna('ãã®ä»–')
    
    category_counts = df_result[f'{target_column}_category'].value_counts()
    print(f"âœ… ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡å®Œäº†:")
    for category, count in category_counts.items():
        print(f"  {category}: {count} ä»¶")
    
    return df_result

def combine_rows(x):
    """è¡Œã®çµåˆ"""
    unique_values = set(str(val) for val in x.dropna() if str(val).lower() != 'nan')
    return '@'.join(unique_values) if unique_values else None

def create_comprehensive_summaries(input_texts):
    """è¤‡æ•°ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€æ‹¬ã§è¦ç´„ã™ã‚‹é–¢æ•°"""
    if not input_texts:
        return []

    summaries = [None] * len(input_texts)
    messages = []
    valid_indices = []
    
    for i, item in enumerate(input_texts):
        if item['text'] and not pd.isna(item['text']):
            text_part = f"ãƒ†ã‚­ã‚¹ãƒˆ{len(valid_indices)+1}: {item['text']}\nç¨®é¡: {item['content_type']}\n---\n"
            messages.append(text_part)
            valid_indices.append(i)
    
    if not messages:
        return summaries

    prompt = f"""ä»¥ä¸‹ã®è¤‡æ•°ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚

{''.join(messages)}

è¦ç´„ãƒ«ãƒ¼ãƒ«:
1. '@'ã§åŒºåˆ‡ã‚‰ã‚ŒãŸå„é …ç›®ã‹ã‚‰äº‹å®Ÿæƒ…å ±ã®ã¿ã‚’æŠ½å‡º
2. è¨˜è¼‰ã•ã‚Œã¦ã„ãªã„æƒ…å ±ã®æ¨æ¸¬ã¯é¿ã‘ã‚‹
3. èª¬æ˜çš„ãªè£…é£¾èªã¯çœã

è¿”ç­”å½¢å¼ï¼š
ãƒ†ã‚­ã‚¹ãƒˆ1ã®è¦ç´„: [è¦ç´„å†…å®¹]
ãƒ†ã‚­ã‚¹ãƒˆ2ã®è¦ç´„: [è¦ç´„å†…å®¹]
...
ï¼ˆå¿…ãšå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã¨åŒã˜æ•°ã®è¦ç´„ã‚’è¿”ã—ã¦ãã ã•ã„ï¼‰"""

    try:
        result = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "è¤‡æ•°ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰äº‹å®Ÿæƒ…å ±ã®ã¿ã‚’æŠ½å‡ºã—ã€ç°¡æ½”ãªè¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        )
        
        response_text = result.choices[0].message.content.strip()
        summary_parts = response_text.split('\n')
        
        summary_count = 0
        for part in summary_parts:
            if part.startswith('ãƒ†ã‚­ã‚¹ãƒˆ') and ': ' in part:
                if summary_count < len(valid_indices):
                    original_index = valid_indices[summary_count]
                    summary = part.split(': ', 1)[1].strip()
                    summaries[original_index] = summary
                    summary_count += 1
    
    except Exception as e:
        print(f"è¦ç´„å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    return summaries

def format_to_dict_list(input_list, content_type):
    """ãƒªã‚¹ãƒˆã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›"""
    return [
        {
            'text': item,
            'content_type': content_type
        }
        for item in input_list
    ]

def summarize_dataframe_column(df, column_name, content_type, batch_size=5):
    """DataFrameã®ç‰¹å®šåˆ—ã‚’è¦ç´„å‡¦ç†"""
    print(f"ğŸ“ {column_name} ã®è¦ç´„å‡¦ç†é–‹å§‹...")
    
    df_result = df.copy()
    
    if column_name not in df_result.columns:
        print(f"âŒ åˆ— '{column_name}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
        return df_result
    
    data_list = df_result[column_name].tolist()
    dict_list = format_to_dict_list(data_list, content_type)
    
    total_batches = (len(dict_list) + batch_size - 1) // batch_size
    
    all_summaries = []
    
    for i in tqdm(range(0, len(dict_list), batch_size), 
                  total=total_batches, 
                  desc=f"{column_name}è¦ç´„å‡¦ç†"):
        batch = dict_list[i:i + batch_size]
        batch_summaries = create_comprehensive_summaries(batch)
        all_summaries.extend(batch_summaries)
    
    df_result[f'{column_name}_summary'] = all_summaries
    
    non_null_summaries = sum(1 for s in all_summaries if s is not None)
    print(f"âœ… è¦ç´„å®Œäº†: {non_null_summaries}/{len(all_summaries)} ä»¶")
    
    return df_result

# =============================================================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†
# =============================================================================

def run_full_pipeline(company_list, max_workers=3):
    """å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ"""
    total_start_time = time.time()
    
    # æ®µéš1: URLåé›†
    all_company_urls, url_df, stats1 = phase1_collect_urls(company_list, max_workers)
    
    # æ®µéš2: ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
    all_company_texts, stats2 = phase2_get_texts(all_company_urls, max_workers)
    
    # æ®µéš3: GPTå‡¦ç†
    gpt_results, stats3 = phase3_gpt_processing(all_company_texts, max_workers)
    
    # æ®µéš4: ãƒ‡ãƒ¼ã‚¿æ•´ç†
    df_result, stats4 = phase4_organize_data(gpt_results)
    
    total_time = time.time() - total_start_time
    
    all_stats = {
        'phase1': stats1,
        'phase2': stats2,
        'phase3': stats3,
        'phase4': stats4,
        'total_time': total_time
    }
    
    print("ğŸ‰ å…¨æ®µéšå‡¦ç†å®Œäº†!")
    print(f"â±ï¸  ç·æ™‚é–“: {total_time:.1f} ç§’")
    print(f"ğŸ“Š æœ€çµ‚ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df_result)}")
    
    return df_result, url_df, all_stats

if __name__ == "__main__":
    company_list = ["ãƒã‚¯ãƒ‹ã‚«", "ä¸‰è±é›»æ©Ÿ", "ã‚¨ãƒ«ã‚¨ãƒ¼ãƒ”ãƒ¼", "ã‚±ã‚¢ãƒœãƒƒãƒˆ", "NTTãƒ‰ã‚³ãƒ¢"]

    print("=== æ®µéšåˆ¥å®Ÿè¡Œ ===")

    # æ®µéš1: URLåé›†
    all_company_urls, url_df, stats1 = phase1_collect_urls(company_list, max_workers=2)
    print(f"æ®µéš1å®Œäº†: {stats1['total_urls']} URLsåé›†")

    # æ®µéš2: ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
    all_company_texts, stats2 = phase2_get_texts(all_company_urls, max_workers=2)
    print(f"æ®µéš2å®Œäº†: {stats2['total_texts']} ãƒ†ã‚­ã‚¹ãƒˆå–å¾—")

    # æ®µéš3: GPTå‡¦ç†
    gpt_results, stats3 = phase3_gpt_processing(all_company_texts, max_workers=2)
    print(f"æ®µéš3å®Œäº†: {stats3['successful_gpt']} æˆåŠŸå‡¦ç†")

    # æ®µéš4: ãƒ‡ãƒ¼ã‚¿æ•´ç†
    final_df, stats4 = phase4_organize_data(gpt_results)
    print(f"æ®µéš4å®Œäº†: {stats4['total_records']} ãƒ¬ã‚³ãƒ¼ãƒ‰")

    print("=== ä¸¦åˆ—å‡¦ç†ç‰ˆ ===")

    updated_df_parallel, failed_cases_parallel, stats = process_final_df3_parallel(
        final_df, 
        max_workers=3  
    )

    print(f"\nğŸ“‹ ä¸¦åˆ—å‡¦ç†çµæœ:")
    print(f"  æˆåŠŸæ›´æ–°: {stats['successful']} è¡Œ")
    print(f"  å¤±æ•—: {len(failed_cases_parallel)} è¡Œ")

    final_df_3 = updated_df_parallel

    print("=== JSONå±•é–‹å‡¦ç† ===")
    expanded_df = expand_json_simple(
        df=final_df_3,
        text_column='text'
    )

    print("=== è£½å“åã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° ===")
    expanded_df_cleaned = clean_dataframe_products(expanded_df, 'json_è£½å“å', batch_size=10)
    
    # ã‚«ãƒ©ãƒ é¸æŠã¨åå‰å¤‰æ›´
    expanded_df_cleaned = expanded_df_cleaned[['ä¼æ¥­å','json_è£½å“å_cleaned','åˆ†é¡', 'URL',  'url',   
                                             'ä½¿ç”¨ç”¨é€”', 'æŠ€è¡“é ˜åŸŸ', 'å”æ¥­å®Ÿç¸¾', 'è£½å“æƒ…å ±',  
                                             'json_ä½¿ç”¨ç”¨é€”', 'json_æŠ€è¡“é ˜åŸŸ','json_å”æ¥­å®Ÿç¸¾', 
                                             'json_è£½å“æƒ…å ±', 'json_å®Ÿè¨¼å®Ÿé¨“', 'json_è£½å“ã®èª¬æ˜']]
    
    expanded_df_cleaned.columns = ['ä¼æ¥­å', 'è£½å“å', 'åˆ†é¡', 'URL', 'url', 'ä½¿ç”¨ç”¨é€”', 'æŠ€è¡“é ˜åŸŸ', 'å”æ¥­å®Ÿç¸¾',
                                  'è£½å“æƒ…å ±',  'json_ä½¿ç”¨ç”¨é€”', 'json_æŠ€è¡“é ˜åŸŸ', 'json_å”æ¥­å®Ÿç¸¾', 'json_è£½å“æƒ…å ±',
                                  'å®Ÿè¨¼å®Ÿé¨“', 'è£½å“ã®èª¬æ˜']

    # ã‚«ãƒ©ãƒ çµåˆ
    expanded_df_colconcat = expanded_df_cleaned.copy()

    expanded_df_colconcat['ä½¿ç”¨ç”¨é€”'] = expanded_df_colconcat['ä½¿ç”¨ç”¨é€”'].astype(str) + "ã€" + expanded_df_colconcat['json_ä½¿ç”¨ç”¨é€”'].astype(str)
    expanded_df_colconcat['æŠ€è¡“é ˜åŸŸ'] = expanded_df_colconcat['æŠ€è¡“é ˜åŸŸ'].astype(str) + "ã€" + expanded_df_colconcat['json_æŠ€è¡“é ˜åŸŸ'].astype(str)
    expanded_df_colconcat['å”æ¥­å®Ÿç¸¾'] = expanded_df_colconcat['å”æ¥­å®Ÿç¸¾'].astype(str) + "ã€" + expanded_df_colconcat['json_å”æ¥­å®Ÿç¸¾'].astype(str)
    expanded_df_colconcat['è£½å“æƒ…å ±'] = expanded_df_colconcat['è£½å“æƒ…å ±'].astype(str) + "ã€" + expanded_df_colconcat['json_è£½å“æƒ…å ±'].astype(str)

    expanded_df_colconcat = expanded_df_colconcat.drop(["json_ä½¿ç”¨ç”¨é€”","json_æŠ€è¡“é ˜åŸŸ","json_å”æ¥­å®Ÿç¸¾","json_è£½å“æƒ…å ±"], axis=1)

    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    expanded_df_groupby = expanded_df_colconcat.groupby(['ä¼æ¥­å', 'è£½å“å'], as_index=False).agg({
        col: lambda x: combine_rows(x) for col in ['åˆ†é¡', 'ä½¿ç”¨ç”¨é€”', 'URL', 'url', 'æŠ€è¡“é ˜åŸŸ', 'å”æ¥­å®Ÿç¸¾','è£½å“æƒ…å ±', 'å®Ÿè¨¼å®Ÿé¨“', 'è£½å“ã®èª¬æ˜']
    })

    # ç©ºã®è£½å“åã‚’é™¤å¤–
    expanded_df_groupby = expanded_df_groupby[expanded_df_groupby["è£½å“å"] != ""]

    print("=== æŠ€è¡“é ˜åŸŸã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡ ===")
    df_with_categories = add_skill_categories_to_dataframe(
        df=expanded_df_groupby,
        target_column='æŠ€è¡“é ˜åŸŸ',
        batch_size=10
    )
    
    df_with_categories = df_with_categories.drop(['æŠ€è¡“é ˜åŸŸ'], axis=1)
    df_with_categories.columns = ['ä¼æ¥­å', 'è£½å“å', 'åˆ†é¡', 'ä½¿ç”¨ç”¨é€”', 'URL', 'url', 'å”æ¥­å®Ÿç¸¾', 'è£½å“æƒ…å ±', 'å®Ÿè¨¼å®Ÿé¨“',
                                 'è£½å“ã®èª¬æ˜', 'æŠ€è¡“é ˜åŸŸ']

    print("=== è¦ç´„å‡¦ç† ===")
    target_columns = {
        'ä½¿ç”¨ç”¨é€”': 'ä½¿ç”¨ç”¨é€”',
        'å”æ¥­å®Ÿç¸¾': 'å”æ¥­å®Ÿç¸¾',
        'è£½å“æƒ…å ±': 'è£½å“æƒ…å ±',
        'å®Ÿè¨¼å®Ÿé¨“': 'å®Ÿè¨¼å®Ÿé¨“',
        'è£½å“ã®èª¬æ˜': 'è£½å“ã®èª¬æ˜'
    }

    df_final = df_with_categories.copy()
    for column, content_type in target_columns.items():
        if column in df_final.columns:
            df_final = summarize_dataframe_column(df_final, column, content_type)

    print("âœ… å…¨å‡¦ç†å®Œäº†!")
    print(f"æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å½¢çŠ¶: {df_final.shape}")
    print("\nä¸»è¦ã‚«ãƒ©ãƒ :")
    for col in df_final.columns:
        print(f"  - {col}")
    df_final = df_final.drop(['ä½¿ç”¨ç”¨é€”','å”æ¥­å®Ÿç¸¾', 'è£½å“æƒ…å ±', 'å®Ÿè¨¼å®Ÿé¨“','è£½å“ã®èª¬æ˜'], axis=1)
    
    # _summaryã‚’å‰Šé™¤ã—ã¦åˆ—åã‚’ã‚¯ãƒªãƒ¼ãƒ³åŒ–
    rename_dict = {}
    for col in df_final.columns:
        if col.endswith('_summary'):
            new_name = col.replace('_summary', '')
            rename_dict[col] = new_name

    df_final = df_final.rename(columns=rename_dict)

    # çµæœã®ä¿å­˜ä¾‹
    df_final.to_csv('./robot_analysis_result.csv', index=False, encoding='utf-8-sig')